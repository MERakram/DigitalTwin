{
    "id": 9,
    "title": "Adversarial Attacks and Defenses on Graph Neural Networks",
    "author": "Dr. Wei Zhang",
    "authorAvatar": "/default-avatar.png",
    "releaseDate": "2024-11-18",
    "category": "Graph Neural Networks",
    "preview": "Graph Neural Networks (GNNs) face vulnerabilities to adversarial attacks—subtle perturbations to graph structure or features that mislead predictions. This article examines attack strategies and robust defenses for secure GNN deployment.",
    "content": "<div><h2>Introduction</h2><p>Graph Neural Networks (GNNs) power applications from social media analysis to fraud detection, relying on graph structure and features for predictions. However, their sensitivity to adversarial attacks—small, deliberate changes to inputs—threatens their reliability. These perturbations can flip classifications, undermining trust in critical systems. Understanding and countering these threats is vital for GNN adoption.</p><p>This article explores adversarial attacks on GNNs, their mechanisms, and defense strategies to fortify these models against malicious manipulation.</p></div><div><h2>Key Concepts</h2><ul><li><strong>Adversarial Perturbations:</strong> Minor edits like edge additions or feature tweaks.</li><li><strong>Targeted vs. Non-Targeted:</strong> Targeted attacks aim for specific errors; non-targeted degrade overall accuracy.</li><li><strong>White-Box vs. Black-Box:</strong> White-box attackers know the model; black-box rely on queries or proxies.</li><li><strong>Evasion vs. Poisoning:</strong> Evasion alters test graphs; poisoning corrupts training data.</li></ul></div><div><h2>Mathematical Formulation</h2><p><strong>Node Feature Attack:</strong></p><pre><code class=\"language-latex\">\\max_{\\delta} \\mathcal{L}(f(A, X + \\delta), y) \\text{ s.t. } \\|\\delta\\|_0 \\leq k</code></pre><p>The attacker maximizes the loss <code>\\mathcal{L}</code> by adding perturbation <code>δ</code> to features <code>X</code>, constrained by sparsity (<code>k</code> changes).</p></div><div><h2>Diagram</h2><p>A graph with an attacked node, showing added edges and altered features:</p><img src=\"https://miro.medium.com/max/1400/1*KZw7ZCtYcD5M_8pMjgAtew.png\" alt=\"Adversarial Attack on Graph\" class=\"w-full my-4 rounded-lg\"></div><div><h2>Attack Types</h2><ul><li><strong>Structure Attacks:</strong> Modify <code>A</code> by adding/removing edges.</li><li><strong>Feature Attacks:</strong> Alter <code>X</code> subtly.</li><li><strong>Node Injection:</strong> Introduce fake nodes to mislead GNNs.</li></ul></div><div><h2>Defenses</h2><ul><li><strong>Adversarial Training:</strong> Include attack examples in training.</li><li><strong>Graph Preprocessing:</strong> Smooth features or prune anomalies.</li><li><strong>Robust Aggregation:</strong> Use median instead of mean to resist outliers.</li></ul></div><div><h2>Advantages</h2><ul><li><strong>Security:</strong> Shields critical applications.</li><li><strong>Robustness:</strong> Enhances model reliability.</li></ul></div><div><h2>Challenges</h2><ul><li><strong>Attack Sophistication:</strong> Evolving threats outpace defenses.</li><li><strong>Cost:</strong> Robustness increases computational demands.</li></ul></div><div><h2>Future Directions</h2><p>Certified defenses and adversarial detection systems promise stronger GNNs, potentially integrating with anomaly detection for real-time protection.</p></div>",
    "citations": [
        "Zügner, D., et al. \"Adversarial Attacks on Neural Networks for Graph Data.\" <em>ACM SIGKDD</em>, 2018.",
        "Dai, E., et al. \"Adversarial Attack on Graph Structured Data.\" <em>ArXiv Preprint arXiv:1806.02371</em>, 2018.",
        "Sun, L., et al. \"Robust Graph Neural Networks via Certified Defenses.\" <em>Neural Networks</em>, 2023."
    ],
    "price": 7500,
    "originalPrice": 10000,
    "isOnSale": true,
    "upvotes": 175,
    "downvotes": 5,
    "comments": 58,
    "imageUrl": "/images/scalability.png"
}